## Towards Monocular Vision based Obstacle Avoidance through Deep Reinforcement Learning

本文研究了机器人在缺少 3D 环境信息的情况下如何使用单目摄像头实现避开障碍物的功能。本文使用的方法是基于竞争结构（dueling architecture）的 double-Q 网络 （D3QN）。使用的数据仅包括单目摄像头采集到的包含 RGB 信息的图像数据。

### 导论

传统的避障方法使用了距离传感器，包括激光扫描传感器或者是超声波传感器。但缺点是感知距离有限，并且成本或是能耗都相对较高。

单目摄像头的优点在于能够在低成本和低能耗的条件下提供较为丰富的环境信息，感知范围也相对较高。但由于只有一个摄像头采集信息，3D的环境数据被转换为 2D 图像，使得感知距离的难度变得更大。

使用单目摄像头进行避障的传统方法分为两步：
- 利用视觉信息推测可以使自身通过的空间，同时预测可能的障碍物。该过程可使用的方法包括视觉 SLAM ，或是基于图像的视觉特性进行分割等的方法。
- 使用传统的路径规划方法，综合上述信息，进行路径规划。
虽然上述方法可以将视觉信息和路径规划进行分离后进行操作，但是该方法需要的参数较多。使用者需要对参数进行调整以适应每个新环境。

随着近年来深度学习在解决计算机视觉问题上的研究与发展，基于深度学习方法进行的避障和路径规划方法越累越多，包括使用卷积神经网络从未处理的输入图像中获得障碍物和路径信息的方法。除了深度学习之外，也有人提出了基于自监督学习和深度强化学习（DRL）的图像处理和避障方法。

本文实现的功能和使用的结构：
- 基于单目摄像头的数据，使用两相深度神经网络实现避障
- 使用基于竞争结构（dueling architecture）的深度double-Q网络（简称为 D3QN），实现在计算资源有限的情况下的告诉端到端学习。


### 基于单目视觉数据和 Deep-Q 网络进行避障

#### 问题定义

避障过程可以认作机器人使用单目摄像头的数据和外界进行交互的决策过程。

在时间点 $t$ ，机器人获取摄像机数据 $x_t$， 并根据摄像机数据选择一个动作 $a_t$， 该动作经奖励函数 （Reward Function）计算后得到奖励信号（reward signal） $r_t$ ，之后机器人获取下一个时刻的摄像机数据 $x_{t+1}$， 并重复上述过程。

算法的目标是获取一个时间段 $T$ 内所有奖励值的和的最大值，即 $R_t = \sum_{\tau =t}^T \gamma^{\tau - t} r_\tau$.

给定一个输入和决策之间的映射关系 $a_t = \pi(x_t)$ , 对于一个状态-动作二元组$(x_t,a_t)$ ，可以计算出对应的 Q 值（Q-Value）。通过在每个时刻都选择最佳决策，同时基于 Bellman 方程可以得出，$t$时刻能够得到的最佳 Q 值是当前的奖励值 $r_t$ 加上 $t+1$ 时刻的衰减后的最佳 Q 值（discounted optimal Q-value）。不同于从较多的状态中计算 Q 值的方法，该方法可以使用深度神经网络估算该 Q 值。这也是该 DQN 网络的工作原理。

#### 竞争网络 （dueling network）和双 Q 值网络（double Q-network， DQN）

传统的 DQN 网络只在卷积层之后设置一层全连接层，用于估计每个状态-动作二元组的 Q 值。而在竞争网络中，使用了两个全连接层分别计算值和优势函数（advantage function），最后合并以计算Q值。

本文使用的 DQN 包括两个网络，一个在线学习网络（online network）和一个目标网络（target network）。在线学习网络在每一步学习过程中都通过反向传播方法更新权重，目标网络的权重取和在线学习网络相同，但是固定一段时间，之后再根据在线学习网络的权重进行更新。
采用这种架构，可以通过在线学习网络进行决策的选择，而目标网络可以用于解决过乐观值估计（overoptimistic value estimation）问题。

更具体的步骤如下：
- 在线学习网络和目标网络都使用结果状态 $x_{t+1}$ 来计算 $t+1$ 时刻的最优 Q 值 $Q‘^*$
- 根据当前的奖励值$r_t$和贴现因子（discount factor） $\gamma$ ，目标网络得出目标值在时刻 $t$ 的值$y$
- 根据当前状态 $x$ 计算误差。误差 = 在线学习网络得到的最优 Q 值 - 目标网络得到的目标值 y
- 反向传播更新权重。

#### 从外观到几何量

DRL 需要大量的数据和时间进行训练，因此该过程适合在模拟环境中进行。但是模拟环境和实际环境仍有一部分的差别，例如外表纹理或是光照等。在训练数据未处理的情况下，通过深度神经网络获得的深度预测数据相较于直接使用深度传感器获得的数据仍有较大的差距。
因此，用于训练过程中的深度图需要加上一些人工噪声或者是模糊处理，经过该处理后，训练得到的神经网络效果有所提升。

#### 建模和训练

文中提出的 D3QN 的结构如下图所示。包括三个卷积层和三个全连接层，用于实现两个竞争结构。

![D3QN Architecture](D3QN_Architecture.png)

本网络中的输出为机器人目标的线速度和角速度。瞬时奖励函数计算方法为

$$ r - v* cos(\omega) * \delta t $$

其中 $v$ 为线速度， $\omega$ 为角速度， $\delta t$ 为一次训练循环的时间，设定为0.2秒。（该设计旨在让机器人保持最高速度，并避免原地旋转）

每段时间的总奖励值为这段时间内每个瞬时奖励值的总和。
当该短时间内发生碰撞时，立即设置一个值为 -10 的惩罚值。
若无碰撞则在达到最大步数时停止，不设置惩罚值。

### 结论

本文提出的 D3QN 神经网络基于深度强化学习算法，只使用单目摄像头的数据便可实现避障功能，经过测试，学习效率和抗图像噪声能力优于一般的 DQN 网络。







